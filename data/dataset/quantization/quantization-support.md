Quantization API Reference 
========================================================================================

torch.ao.quantization 
------------------------------------------------------------------------------

This module contains Eager mode quantization APIs. 

### Top level APIs 

| [`quantize`](generated/torch.ao.quantization.quantize.html#torch.ao.quantization.quantize "torch.ao.quantization.quantize") | Quantize the input float model with post training static quantization. |
| --- | --- |
| [`quantize_dynamic`](generated/torch.ao.quantization.quantize_dynamic.html#torch.ao.quantization.quantize_dynamic "torch.ao.quantization.quantize_dynamic") | Converts a float model to dynamic (i.e. |
| [`quantize_qat`](generated/torch.ao.quantization.quantize_qat.html#torch.ao.quantization.quantize_qat "torch.ao.quantization.quantize_qat") | Do quantization aware training and output a quantized model |
| [`prepare`](generated/torch.ao.quantization.prepare.html#torch.ao.quantization.prepare "torch.ao.quantization.prepare") | Prepares a copy of the model for quantization calibration or quantization-aware training. |
| [`prepare_qat`](generated/torch.ao.quantization.prepare_qat.html#torch.ao.quantization.prepare_qat "torch.ao.quantization.prepare_qat") | Prepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version. |
| [`convert`](generated/torch.ao.quantization.convert.html#torch.ao.quantization.convert "torch.ao.quantization.convert") | Converts submodules in input module to a different module according to mapping  by calling from_float  method on the target module class. |

### Preparing model for quantization 

| [`fuse_modules.fuse_modules`](generated/torch.ao.quantization.fuse_modules.fuse_modules.html#torch.ao.quantization.fuse_modules.fuse_modules "torch.ao.quantization.fuse_modules.fuse_modules") | Fuse a list of modules into a single module. |
| --- | --- |
| [`QuantStub`](generated/torch.ao.quantization.QuantStub.html#torch.ao.quantization.QuantStub "torch.ao.quantization.QuantStub") | Quantize stub module, before calibration, this is same as an observer, it will be swapped as nnq.Quantize  in convert  . |
| [`DeQuantStub`](generated/torch.ao.quantization.DeQuantStub.html#torch.ao.quantization.DeQuantStub "torch.ao.quantization.DeQuantStub") | Dequantize stub module, before calibration, this is same as identity, this will be swapped as nnq.DeQuantize  in convert  . |
| [`QuantWrapper`](generated/torch.ao.quantization.QuantWrapper.html#torch.ao.quantization.QuantWrapper "torch.ao.quantization.QuantWrapper") | A wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules. |
| [`add_quant_dequant`](generated/torch.ao.quantization.add_quant_dequant.html#torch.ao.quantization.add_quant_dequant "torch.ao.quantization.add_quant_dequant") | Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well. |

### Utility functions 

| [`swap_module`](generated/torch.ao.quantization.swap_module.html#torch.ao.quantization.swap_module "torch.ao.quantization.swap_module") | Swaps the module if it has a quantized counterpart and it has an observer  attached. |
| --- | --- |
| [`propagate_qconfig_`](generated/torch.ao.quantization.propagate_qconfig_.html#torch.ao.quantization.propagate_qconfig_ "torch.ao.quantization.propagate_qconfig_") | Propagate qconfig through the module hierarchy and assign qconfig  attribute on each leaf module |
| [`default_eval_fn`](generated/torch.ao.quantization.default_eval_fn.html#torch.ao.quantization.default_eval_fn "torch.ao.quantization.default_eval_fn") | Define the default evaluation function. |

torch.ao.quantization.quantize_fx 
-------------------------------------------------------------------------------------------------------

This module contains FX graph mode quantization APIs (prototype). 

| [`prepare_fx`](generated/torch.ao.quantization.quantize_fx.prepare_fx.html#torch.ao.quantization.quantize_fx.prepare_fx "torch.ao.quantization.quantize_fx.prepare_fx") | Prepare a model for post training quantization |
| --- | --- |
| [`prepare_qat_fx`](generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html#torch.ao.quantization.quantize_fx.prepare_qat_fx "torch.ao.quantization.quantize_fx.prepare_qat_fx") | Prepare a model for quantization aware training |
| [`convert_fx`](generated/torch.ao.quantization.quantize_fx.convert_fx.html#torch.ao.quantization.quantize_fx.convert_fx "torch.ao.quantization.quantize_fx.convert_fx") | Convert a calibrated or trained model to a quantized model |
| [`fuse_fx`](generated/torch.ao.quantization.quantize_fx.fuse_fx.html#torch.ao.quantization.quantize_fx.fuse_fx "torch.ao.quantization.quantize_fx.fuse_fx") | Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode. |

torch.ao.quantization.qconfig_mapping 
---------------------------------------------------------------------------------------------------------------

This module contains QConfigMapping for configuring FX graph mode quantization. 

| [`QConfigMapping`](generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping "torch.ao.quantization.qconfig_mapping.QConfigMapping") | Mapping from model ops to `torch.ao.quantization.QConfig`  s. |
| --- | --- |
| [`get_default_qconfig_mapping`](generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping.html#torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping "torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping") | Return the default QConfigMapping for post training quantization. |
| [`get_default_qat_qconfig_mapping`](generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping.html#torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping "torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping") | Return the default QConfigMapping for quantization aware training. |

torch.ao.quantization.backend_config 
-------------------------------------------------------------------------------------------------------------

This module contains BackendConfig, a config object that defines how quantization is supported
in a backend. Currently only used by FX Graph Mode Quantization, but we may extend Eager Mode
Quantization to work with this as well. 

| [`BackendConfig`](generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig "torch.ao.quantization.backend_config.BackendConfig") | Config that defines the set of patterns that can be quantized on a given backend, and how reference quantized models can be produced from these patterns. |
| --- | --- |
| [`BackendPatternConfig`](generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig "torch.ao.quantization.backend_config.BackendPatternConfig") | Config object that specifies quantization behavior for a given operator pattern. |
| [`DTypeConfig`](generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig "torch.ao.quantization.backend_config.DTypeConfig") | Config object that specifies the supported data types passed as arguments to quantize ops in the reference model spec, for input and output activations, weights, and biases. |
| [`DTypeWithConstraints`](generated/torch.ao.quantization.backend_config.DTypeWithConstraints.html#torch.ao.quantization.backend_config.DTypeWithConstraints "torch.ao.quantization.backend_config.DTypeWithConstraints") | Config for specifying additional constraints for a given dtype, such as quantization value ranges, scale value ranges, and fixed quantization params, to be used in [`DTypeConfig`](generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig "torch.ao.quantization.backend_config.DTypeConfig")  . |
| [`ObservationType`](generated/torch.ao.quantization.backend_config.ObservationType.html#torch.ao.quantization.backend_config.ObservationType "torch.ao.quantization.backend_config.ObservationType") | An enum that represents different ways of how an operator/operator pattern should be observed |

torch.ao.quantization.fx.custom_config 
-----------------------------------------------------------------------------------------------------------------

This module contains a few CustomConfig classes that’s used in both eager mode and FX graph mode quantization 

| [`FuseCustomConfig`](generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html#torch.ao.quantization.fx.custom_config.FuseCustomConfig "torch.ao.quantization.fx.custom_config.FuseCustomConfig") | Custom configuration for [`fuse_fx()`](generated/torch.ao.quantization.quantize_fx.fuse_fx.html#torch.ao.quantization.quantize_fx.fuse_fx "torch.ao.quantization.quantize_fx.fuse_fx")  . |
| --- | --- |
| [`PrepareCustomConfig`](generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig "torch.ao.quantization.fx.custom_config.PrepareCustomConfig") | Custom configuration for [`prepare_fx()`](generated/torch.ao.quantization.quantize_fx.prepare_fx.html#torch.ao.quantization.quantize_fx.prepare_fx "torch.ao.quantization.quantize_fx.prepare_fx")  and [`prepare_qat_fx()`](generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html#torch.ao.quantization.quantize_fx.prepare_qat_fx "torch.ao.quantization.quantize_fx.prepare_qat_fx")  . |
| [`ConvertCustomConfig`](generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig "torch.ao.quantization.fx.custom_config.ConvertCustomConfig") | Custom configuration for [`convert_fx()`](generated/torch.ao.quantization.quantize_fx.convert_fx.html#torch.ao.quantization.quantize_fx.convert_fx "torch.ao.quantization.quantize_fx.convert_fx")  . |
| [`StandaloneModuleConfigEntry`](generated/torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry.html#torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry "torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry") |  |

torch.ao.quantization.quantizer 
---------------------------------------------------------------------------------------------------------

torch.ao.quantization.pt2e (quantization in pytorch 2.0 export implementation) 
----------------------------------------------------------------------------------------------------------------------------------------------------

torch.ao.quantization.pt2e.export_utils 
-------------------------------------------------------------------------------------------------------------------

| [`model_is_exported`](generated/torch.ao.quantization.pt2e.export_utils.model_is_exported.html#torch.ao.quantization.pt2e.export_utils.model_is_exported "torch.ao.quantization.pt2e.export_utils.model_is_exported") | Return True if the torch.nn.Module  was exported, False otherwise (e.g. |
| --- | --- |

torch.ao.quantization.pt2e.lowering 
----------------------------------------------------------------------------------------------------------

| [`lower_pt2e_quantized_to_x86`](generated/torch.ao.quantization.pt2e.lowering.lower_pt2e_quantized_to_x86.html#torch.ao.quantization.pt2e.lowering.lower_pt2e_quantized_to_x86 "torch.ao.quantization.pt2e.lowering.lower_pt2e_quantized_to_x86") | Lower a PT2E-qantized model to x86 backend. |
| --- | --- |

PT2 Export (pt2e) Numeric Debugger 
------------------------------------------------------------------------------------------------------

| [`generate_numeric_debug_handle`](generated/torch.ao.quantization.generate_numeric_debug_handle.html#torch.ao.quantization.generate_numeric_debug_handle "torch.ao.quantization.generate_numeric_debug_handle") | Attach numeric_debug_handle_id for all nodes in the graph module of the given ExportedProgram, like conv2d, squeeze, conv1d, etc, except for placeholder. |
| --- | --- |
| [`CUSTOM_KEY`](generated/torch.ao.quantization.CUSTOM_KEY.html#torch.ao.quantization.CUSTOM_KEY "torch.ao.quantization.CUSTOM_KEY") | str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str |
| [`NUMERIC_DEBUG_HANDLE_KEY`](generated/torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY.html#torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY "torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY") | str(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str |
| [`prepare_for_propagation_comparison`](generated/torch.ao.quantization.prepare_for_propagation_comparison.html#torch.ao.quantization.prepare_for_propagation_comparison "torch.ao.quantization.prepare_for_propagation_comparison") | Add output loggers to node that has numeric_debug_handle |
| [`extract_results_from_loggers`](generated/torch.ao.quantization.extract_results_from_loggers.html#torch.ao.quantization.extract_results_from_loggers "torch.ao.quantization.extract_results_from_loggers") | For a given model, extract the tensors stats and related information for each debug handle. |
| [`compare_results`](generated/torch.ao.quantization.compare_results.html#torch.ao.quantization.compare_results "torch.ao.quantization.compare_results") | Given two dict mapping from debug_handle_id  (int) to list of tensors return a map from debug_handle_id  to NodeAccuracySummary  that contains comparison information like SQNR, MSE etc. |

torch (quantization related functions) 
--------------------------------------------------------------------------------------------------------------

This describes the quantization related functions of the `torch`  namespace. 

| [`quantize_per_tensor`](generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor "torch.quantize_per_tensor") | Converts a float tensor to a quantized tensor with given scale and zero point. |
| --- | --- |
| [`quantize_per_channel`](generated/torch.quantize_per_channel.html#torch.quantize_per_channel "torch.quantize_per_channel") | Converts a float tensor to a per-channel quantized tensor with given scales and zero points. |
| [`dequantize`](generated/torch.dequantize.html#torch.dequantize "torch.dequantize") | Returns an fp32 Tensor by dequantizing a quantized Tensor |

torch.Tensor (quantization related methods) 
------------------------------------------------------------------------------------------------------------------------

Quantized Tensors support a limited subset of data manipulation methods of the
regular full-precision tensor. 

| [`view`](generated/torch.Tensor.view.html#torch.Tensor.view "torch.Tensor.view") | Returns a new tensor with the same data as the `self`  tensor but of a different [`shape`](generated/torch.Tensor.shape.html#torch.Tensor.shape "torch.Tensor.shape")  . |
| --- | --- |
| [`as_strided`](generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided "torch.Tensor.as_strided") | See [`torch.as_strided()`](generated/torch.as_strided.html#torch.as_strided "torch.as_strided") |
| [`expand`](generated/torch.Tensor.expand.html#torch.Tensor.expand "torch.Tensor.expand") | Returns a new view of the `self`  tensor with singleton dimensions expanded to a larger size. |
| [`flatten`](generated/torch.Tensor.flatten.html#torch.Tensor.flatten "torch.Tensor.flatten") | See [`torch.flatten()`](generated/torch.flatten.html#torch.flatten "torch.flatten") |
| [`select`](generated/torch.Tensor.select.html#torch.Tensor.select "torch.Tensor.select") | See [`torch.select()`](generated/torch.select.html#torch.select "torch.select") |
| [`ne`](generated/torch.Tensor.ne.html#torch.Tensor.ne "torch.Tensor.ne") | See [`torch.ne()`](generated/torch.ne.html#torch.ne "torch.ne")  . |
| [`eq`](generated/torch.Tensor.eq.html#torch.Tensor.eq "torch.Tensor.eq") | See [`torch.eq()`](generated/torch.eq.html#torch.eq "torch.eq") |
| [`ge`](generated/torch.Tensor.ge.html#torch.Tensor.ge "torch.Tensor.ge") | See [`torch.ge()`](generated/torch.ge.html#torch.ge "torch.ge")  . |
| [`le`](generated/torch.Tensor.le.html#torch.Tensor.le "torch.Tensor.le") | See [`torch.le()`](generated/torch.le.html#torch.le "torch.le")  . |
| [`gt`](generated/torch.Tensor.gt.html#torch.Tensor.gt "torch.Tensor.gt") | See [`torch.gt()`](generated/torch.gt.html#torch.gt "torch.gt")  . |
| [`lt`](generated/torch.Tensor.lt.html#torch.Tensor.lt "torch.Tensor.lt") | See [`torch.lt()`](generated/torch.lt.html#torch.lt "torch.lt")  . |
| [`copy_`](generated/torch.Tensor.copy_.html#torch.Tensor.copy_ "torch.Tensor.copy_") | Copies the elements from `src`  into `self`  tensor and returns `self`  . |
| [`clone`](generated/torch.Tensor.clone.html#torch.Tensor.clone "torch.Tensor.clone") | See [`torch.clone()`](generated/torch.clone.html#torch.clone "torch.clone") |
| [`dequantize`](generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize "torch.Tensor.dequantize") | Given a quantized Tensor, dequantize it and return the dequantized float Tensor. |
| [`equal`](generated/torch.Tensor.equal.html#torch.Tensor.equal "torch.Tensor.equal") | See [`torch.equal()`](generated/torch.equal.html#torch.equal "torch.equal") |
| [`int_repr`](generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr "torch.Tensor.int_repr") | Given a quantized Tensor, `self.int_repr()`  returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor. |
| [`max`](generated/torch.Tensor.max.html#torch.Tensor.max "torch.Tensor.max") | See [`torch.max()`](generated/torch.max.html#torch.max "torch.max") |
| [`mean`](generated/torch.Tensor.mean.html#torch.Tensor.mean "torch.Tensor.mean") | See [`torch.mean()`](generated/torch.mean.html#torch.mean "torch.mean") |
| [`min`](generated/torch.Tensor.min.html#torch.Tensor.min "torch.Tensor.min") | See [`torch.min()`](generated/torch.min.html#torch.min "torch.min") |
| [`q_scale`](generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale "torch.Tensor.q_scale") | Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer(). |
| [`q_zero_point`](generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point "torch.Tensor.q_zero_point") | Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer(). |
| [`q_per_channel_scales`](generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales "torch.Tensor.q_per_channel_scales") | Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. |
| [`q_per_channel_zero_points`](generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points "torch.Tensor.q_per_channel_zero_points") | Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. |
| [`q_per_channel_axis`](generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis "torch.Tensor.q_per_channel_axis") | Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied. |
| [`resize_`](generated/torch.Tensor.resize_.html#torch.Tensor.resize_ "torch.Tensor.resize_") | Resizes `self`  tensor to the specified size. |
| [`sort`](generated/torch.Tensor.sort.html#torch.Tensor.sort "torch.Tensor.sort") | See [`torch.sort()`](generated/torch.sort.html#torch.sort "torch.sort") |
| [`topk`](generated/torch.Tensor.topk.html#torch.Tensor.topk "torch.Tensor.topk") | See [`torch.topk()`](generated/torch.topk.html#torch.topk "torch.topk") |

torch.ao.quantization.observer 
------------------------------------------------------------------------------------------------

This module contains observers which are used to collect statistics about
the values observed during calibration (PTQ) or training (QAT). 

| [`ObserverBase`](generated/torch.ao.quantization.observer.ObserverBase.html#torch.ao.quantization.observer.ObserverBase "torch.ao.quantization.observer.ObserverBase") | Base observer Module. |
| --- | --- |
| [`MinMaxObserver`](generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver "torch.ao.quantization.observer.MinMaxObserver") | Observer module for computing the quantization parameters based on the running min and max values. |
| [`MovingAverageMinMaxObserver`](generated/torch.ao.quantization.observer.MovingAverageMinMaxObserver.html#torch.ao.quantization.observer.MovingAverageMinMaxObserver "torch.ao.quantization.observer.MovingAverageMinMaxObserver") | Observer module for computing the quantization parameters based on the moving average of the min and max values. |
| [`PerChannelMinMaxObserver`](generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html#torch.ao.quantization.observer.PerChannelMinMaxObserver "torch.ao.quantization.observer.PerChannelMinMaxObserver") | Observer module for computing the quantization parameters based on the running per channel min and max values. |
| [`MovingAveragePerChannelMinMaxObserver`](generated/torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.html#torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver "torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver") | Observer module for computing the quantization parameters based on the running per channel min and max values. |
| [`HistogramObserver`](generated/torch.ao.quantization.observer.HistogramObserver.html#torch.ao.quantization.observer.HistogramObserver "torch.ao.quantization.observer.HistogramObserver") | The module records the running histogram of tensor values along with min/max values. |
| [`PlaceholderObserver`](generated/torch.ao.quantization.observer.PlaceholderObserver.html#torch.ao.quantization.observer.PlaceholderObserver "torch.ao.quantization.observer.PlaceholderObserver") | Observer that doesn't do anything and just passes its configuration to the quantized module's `.from_float()`  . |
| [`RecordingObserver`](generated/torch.ao.quantization.observer.RecordingObserver.html#torch.ao.quantization.observer.RecordingObserver "torch.ao.quantization.observer.RecordingObserver") | The module is mainly for debug and records the tensor values during runtime. |
| [`NoopObserver`](generated/torch.ao.quantization.observer.NoopObserver.html#torch.ao.quantization.observer.NoopObserver "torch.ao.quantization.observer.NoopObserver") | Observer that doesn't do anything and just passes its configuration to the quantized module's `.from_float()`  . |
| [`get_observer_state_dict`](generated/torch.ao.quantization.observer.get_observer_state_dict.html#torch.ao.quantization.observer.get_observer_state_dict "torch.ao.quantization.observer.get_observer_state_dict") | Returns the state dict corresponding to the observer stats. |
| [`load_observer_state_dict`](generated/torch.ao.quantization.observer.load_observer_state_dict.html#torch.ao.quantization.observer.load_observer_state_dict "torch.ao.quantization.observer.load_observer_state_dict") | Given input model and a state_dict containing model observer stats, load the stats back into the model. |
| [`default_observer`](generated/torch.ao.quantization.observer.default_observer.html#torch.ao.quantization.observer.default_observer "torch.ao.quantization.observer.default_observer") | Default observer for static quantization, usually used for debugging. |
| [`default_placeholder_observer`](generated/torch.ao.quantization.observer.default_placeholder_observer.html#torch.ao.quantization.observer.default_placeholder_observer "torch.ao.quantization.observer.default_placeholder_observer") | Default placeholder observer, usually used for quantization to torch.float16. |
| [`default_debug_observer`](generated/torch.ao.quantization.observer.default_debug_observer.html#torch.ao.quantization.observer.default_debug_observer "torch.ao.quantization.observer.default_debug_observer") | Default debug-only observer. |
| [`default_weight_observer`](generated/torch.ao.quantization.observer.default_weight_observer.html#torch.ao.quantization.observer.default_weight_observer "torch.ao.quantization.observer.default_weight_observer") | Default weight observer. |
| [`default_histogram_observer`](generated/torch.ao.quantization.observer.default_histogram_observer.html#torch.ao.quantization.observer.default_histogram_observer "torch.ao.quantization.observer.default_histogram_observer") | Default histogram observer, usually used for PTQ. |
| [`default_per_channel_weight_observer`](generated/torch.ao.quantization.observer.default_per_channel_weight_observer.html#torch.ao.quantization.observer.default_per_channel_weight_observer "torch.ao.quantization.observer.default_per_channel_weight_observer") | Default per-channel weight observer, usually used on backends where per-channel weight quantization is supported, such as fbgemm  . |
| [`default_dynamic_quant_observer`](generated/torch.ao.quantization.observer.default_dynamic_quant_observer.html#torch.ao.quantization.observer.default_dynamic_quant_observer "torch.ao.quantization.observer.default_dynamic_quant_observer") | Default observer for dynamic quantization. |
| [`default_float_qparams_observer`](generated/torch.ao.quantization.observer.default_float_qparams_observer.html#torch.ao.quantization.observer.default_float_qparams_observer "torch.ao.quantization.observer.default_float_qparams_observer") | Default observer for a floating point zero-point. |
| [`AffineQuantizedObserverBase`](generated/torch.ao.quantization.observer.AffineQuantizedObserverBase.html#torch.ao.quantization.observer.AffineQuantizedObserverBase "torch.ao.quantization.observer.AffineQuantizedObserverBase") | Observer module for affine quantization ( [pytorch/ao](https://github.com/pytorch/ao/tree/main/torchao/quantization#affine-quantization)  ) |
| [`Granularity`](generated/torch.ao.quantization.observer.Granularity.html#torch.ao.quantization.observer.Granularity "torch.ao.quantization.observer.Granularity") | Base class for representing the granularity of quantization. |
| [`MappingType`](generated/torch.ao.quantization.observer.MappingType.html#torch.ao.quantization.observer.MappingType "torch.ao.quantization.observer.MappingType") | How floating point number is mapped to integer number |
| [`PerAxis`](generated/torch.ao.quantization.observer.PerAxis.html#torch.ao.quantization.observer.PerAxis "torch.ao.quantization.observer.PerAxis") | Represents per-axis granularity in quantization. |
| [`PerBlock`](generated/torch.ao.quantization.observer.PerBlock.html#torch.ao.quantization.observer.PerBlock "torch.ao.quantization.observer.PerBlock") | Represents per-block granularity in quantization. |
| [`PerGroup`](generated/torch.ao.quantization.observer.PerGroup.html#torch.ao.quantization.observer.PerGroup "torch.ao.quantization.observer.PerGroup") | Represents per-channel group granularity in quantization. |
| [`PerRow`](generated/torch.ao.quantization.observer.PerRow.html#torch.ao.quantization.observer.PerRow "torch.ao.quantization.observer.PerRow") | Represents row-wise granularity in quantization. |
| [`PerTensor`](generated/torch.ao.quantization.observer.PerTensor.html#torch.ao.quantization.observer.PerTensor "torch.ao.quantization.observer.PerTensor") | Represents per-tensor granularity in quantization. |
| [`PerToken`](generated/torch.ao.quantization.observer.PerToken.html#torch.ao.quantization.observer.PerToken "torch.ao.quantization.observer.PerToken") | Represents per-token granularity in quantization. |
| [`TorchAODType`](generated/torch.ao.quantization.observer.TorchAODType.html#torch.ao.quantization.observer.TorchAODType "torch.ao.quantization.observer.TorchAODType") | Placeholder for dtypes that do not exist in PyTorch core yet. |
| [`ZeroPointDomain`](generated/torch.ao.quantization.observer.ZeroPointDomain.html#torch.ao.quantization.observer.ZeroPointDomain "torch.ao.quantization.observer.ZeroPointDomain") | Enum that indicate whether zero_point is in integer domain or floating point domain |
| [`get_block_size`](generated/torch.ao.quantization.observer.get_block_size.html#torch.ao.quantization.observer.get_block_size "torch.ao.quantization.observer.get_block_size") | Get the block size based on the input shape and granularity type. |

torch.ao.quantization.fake_quantize 
-----------------------------------------------------------------------------------------------------------

This module implements modules which are used to perform fake quantization
during QAT. 

| [`FakeQuantizeBase`](generated/torch.ao.quantization.fake_quantize.FakeQuantizeBase.html#torch.ao.quantization.fake_quantize.FakeQuantizeBase "torch.ao.quantization.fake_quantize.FakeQuantizeBase") | Base fake quantize module. |
| --- | --- |
| [`FakeQuantize`](generated/torch.ao.quantization.fake_quantize.FakeQuantize.html#torch.ao.quantization.fake_quantize.FakeQuantize "torch.ao.quantization.fake_quantize.FakeQuantize") | Simulate the quantize and dequantize operations in training time. |
| [`FixedQParamsFakeQuantize`](generated/torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.html#torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize "torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize") | Simulate quantize and dequantize in training time. |
| [`FusedMovingAvgObsFakeQuantize`](generated/torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.html#torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize "torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize") | Define a fused module to observe the tensor. |
| [`default_fake_quant`](generated/torch.ao.quantization.fake_quantize.default_fake_quant.html#torch.ao.quantization.fake_quantize.default_fake_quant "torch.ao.quantization.fake_quantize.default_fake_quant") | Default fake_quant for activations. |
| [`default_weight_fake_quant`](generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant.html#torch.ao.quantization.fake_quantize.default_weight_fake_quant "torch.ao.quantization.fake_quantize.default_weight_fake_quant") | Default fake_quant for weights. |
| [`default_per_channel_weight_fake_quant`](generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant.html#torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant "torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant") | Default fake_quant for per-channel weights. |
| [`default_histogram_fake_quant`](generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant.html#torch.ao.quantization.fake_quantize.default_histogram_fake_quant "torch.ao.quantization.fake_quantize.default_histogram_fake_quant") | Fake_quant for activations using a histogram.. |
| [`default_fused_act_fake_quant`](generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_act_fake_quant "torch.ao.quantization.fake_quantize.default_fused_act_fake_quant") | Fused version of default_fake_quant  , with improved performance. |
| [`default_fused_wt_fake_quant`](generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant "torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant") | Fused version of default_weight_fake_quant  , with improved performance. |
| [`default_fused_per_channel_wt_fake_quant`](generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant "torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant") | Fused version of default_per_channel_weight_fake_quant  , with improved performance. |
| [`disable_fake_quant`](generated/torch.ao.quantization.fake_quantize.disable_fake_quant.html#torch.ao.quantization.fake_quantize.disable_fake_quant "torch.ao.quantization.fake_quantize.disable_fake_quant") | Disable fake quantization for the module. |
| [`enable_fake_quant`](generated/torch.ao.quantization.fake_quantize.enable_fake_quant.html#torch.ao.quantization.fake_quantize.enable_fake_quant "torch.ao.quantization.fake_quantize.enable_fake_quant") | Enable fake quantization for the module. |
| [`disable_observer`](generated/torch.ao.quantization.fake_quantize.disable_observer.html#torch.ao.quantization.fake_quantize.disable_observer "torch.ao.quantization.fake_quantize.disable_observer") | Disable observation for this module. |
| [`enable_observer`](generated/torch.ao.quantization.fake_quantize.enable_observer.html#torch.ao.quantization.fake_quantize.enable_observer "torch.ao.quantization.fake_quantize.enable_observer") | Enable observation for this module. |

torch.ao.quantization.qconfig 
----------------------------------------------------------------------------------------------

This module defines `QConfig`  objects which are used
to configure quantization settings for individual ops. 

| [`QConfig`](generated/torch.ao.quantization.qconfig.QConfig.html#torch.ao.quantization.qconfig.QConfig "torch.ao.quantization.qconfig.QConfig") | Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively. |
| --- | --- |
| [`default_qconfig`](generated/torch.ao.quantization.qconfig.default_qconfig.html#torch.ao.quantization.qconfig.default_qconfig "torch.ao.quantization.qconfig.default_qconfig") | Default qconfig configuration. |
| [`default_debug_qconfig`](generated/torch.ao.quantization.qconfig.default_debug_qconfig.html#torch.ao.quantization.qconfig.default_debug_qconfig "torch.ao.quantization.qconfig.default_debug_qconfig") | Default qconfig configuration for debugging. |
| [`default_per_channel_qconfig`](generated/torch.ao.quantization.qconfig.default_per_channel_qconfig.html#torch.ao.quantization.qconfig.default_per_channel_qconfig "torch.ao.quantization.qconfig.default_per_channel_qconfig") | Default qconfig configuration for per channel weight quantization. |
| [`default_dynamic_qconfig`](generated/torch.ao.quantization.qconfig.default_dynamic_qconfig.html#torch.ao.quantization.qconfig.default_dynamic_qconfig "torch.ao.quantization.qconfig.default_dynamic_qconfig") | Default dynamic qconfig. |
| [`float16_dynamic_qconfig`](generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig.html#torch.ao.quantization.qconfig.float16_dynamic_qconfig "torch.ao.quantization.qconfig.float16_dynamic_qconfig") | Dynamic qconfig with weights quantized to torch.float16  . |
| [`float16_static_qconfig`](generated/torch.ao.quantization.qconfig.float16_static_qconfig.html#torch.ao.quantization.qconfig.float16_static_qconfig "torch.ao.quantization.qconfig.float16_static_qconfig") | Dynamic qconfig with both activations and weights quantized to torch.float16  . |
| [`per_channel_dynamic_qconfig`](generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig.html#torch.ao.quantization.qconfig.per_channel_dynamic_qconfig "torch.ao.quantization.qconfig.per_channel_dynamic_qconfig") | Dynamic qconfig with weights quantized per channel. |
| [`float_qparams_weight_only_qconfig`](generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig.html#torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig "torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig") | Dynamic qconfig with weights quantized with a floating point zero_point. |
| [`default_qat_qconfig`](generated/torch.ao.quantization.qconfig.default_qat_qconfig.html#torch.ao.quantization.qconfig.default_qat_qconfig "torch.ao.quantization.qconfig.default_qat_qconfig") | Default qconfig for QAT. |
| [`default_weight_only_qconfig`](generated/torch.ao.quantization.qconfig.default_weight_only_qconfig.html#torch.ao.quantization.qconfig.default_weight_only_qconfig "torch.ao.quantization.qconfig.default_weight_only_qconfig") | Default qconfig for quantizing weights only. |
| [`default_activation_only_qconfig`](generated/torch.ao.quantization.qconfig.default_activation_only_qconfig.html#torch.ao.quantization.qconfig.default_activation_only_qconfig "torch.ao.quantization.qconfig.default_activation_only_qconfig") | Default qconfig for quantizing activations only. |
| [`default_qat_qconfig_v2`](generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2.html#torch.ao.quantization.qconfig.default_qat_qconfig_v2 "torch.ao.quantization.qconfig.default_qat_qconfig_v2") | Fused version of default_qat_config  , has performance benefits. |

torch.ao.nn.intrinsic 
-------------------------------------------------------------------------------------

This module implements the combined (fused) modules conv + relu which can
then be quantized. 

| [`ConvReLU1d`](generated/torch.ao.nn.intrinsic.ConvReLU1d.html#torch.ao.nn.intrinsic.ConvReLU1d "torch.ao.nn.intrinsic.ConvReLU1d") | This is a sequential container which calls the Conv1d and ReLU modules. |
| --- | --- |
| [`ConvReLU2d`](generated/torch.ao.nn.intrinsic.ConvReLU2d.html#torch.ao.nn.intrinsic.ConvReLU2d "torch.ao.nn.intrinsic.ConvReLU2d") | This is a sequential container which calls the Conv2d and ReLU modules. |
| [`ConvReLU3d`](generated/torch.ao.nn.intrinsic.ConvReLU3d.html#torch.ao.nn.intrinsic.ConvReLU3d "torch.ao.nn.intrinsic.ConvReLU3d") | This is a sequential container which calls the Conv3d and ReLU modules. |
| [`LinearReLU`](generated/torch.ao.nn.intrinsic.LinearReLU.html#torch.ao.nn.intrinsic.LinearReLU "torch.ao.nn.intrinsic.LinearReLU") | This is a sequential container which calls the Linear and ReLU modules. |
| [`ConvBn1d`](generated/torch.ao.nn.intrinsic.ConvBn1d.html#torch.ao.nn.intrinsic.ConvBn1d "torch.ao.nn.intrinsic.ConvBn1d") | This is a sequential container which calls the Conv 1d and Batch Norm 1d modules. |
| [`ConvBn2d`](generated/torch.ao.nn.intrinsic.ConvBn2d.html#torch.ao.nn.intrinsic.ConvBn2d "torch.ao.nn.intrinsic.ConvBn2d") | This is a sequential container which calls the Conv 2d and Batch Norm 2d modules. |
| [`ConvBn3d`](generated/torch.ao.nn.intrinsic.ConvBn3d.html#torch.ao.nn.intrinsic.ConvBn3d "torch.ao.nn.intrinsic.ConvBn3d") | This is a sequential container which calls the Conv 3d and Batch Norm 3d modules. |
| [`ConvBnReLU1d`](generated/torch.ao.nn.intrinsic.ConvBnReLU1d.html#torch.ao.nn.intrinsic.ConvBnReLU1d "torch.ao.nn.intrinsic.ConvBnReLU1d") | This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules. |
| [`ConvBnReLU2d`](generated/torch.ao.nn.intrinsic.ConvBnReLU2d.html#torch.ao.nn.intrinsic.ConvBnReLU2d "torch.ao.nn.intrinsic.ConvBnReLU2d") | This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. |
| [`ConvBnReLU3d`](generated/torch.ao.nn.intrinsic.ConvBnReLU3d.html#torch.ao.nn.intrinsic.ConvBnReLU3d "torch.ao.nn.intrinsic.ConvBnReLU3d") | This is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules. |
| [`BNReLU2d`](generated/torch.ao.nn.intrinsic.BNReLU2d.html#torch.ao.nn.intrinsic.BNReLU2d "torch.ao.nn.intrinsic.BNReLU2d") | This is a sequential container which calls the BatchNorm 2d and ReLU modules. |
| [`BNReLU3d`](generated/torch.ao.nn.intrinsic.BNReLU3d.html#torch.ao.nn.intrinsic.BNReLU3d "torch.ao.nn.intrinsic.BNReLU3d") | This is a sequential container which calls the BatchNorm 3d and ReLU modules. |

torch.ao.nn.intrinsic.qat 
---------------------------------------------------------------------------------------------

This module implements the versions of those fused operations needed for
quantization aware training. 

| [`LinearReLU`](generated/torch.ao.nn.intrinsic.qat.LinearReLU.html#torch.ao.nn.intrinsic.qat.LinearReLU "torch.ao.nn.intrinsic.qat.LinearReLU") | A LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training. |
| --- | --- |
| [`ConvBn1d`](generated/torch.ao.nn.intrinsic.qat.ConvBn1d.html#torch.ao.nn.intrinsic.qat.ConvBn1d "torch.ao.nn.intrinsic.qat.ConvBn1d") | A ConvBn1d module is a module fused from Conv1d and BatchNorm1d, attached with FakeQuantize modules for weight, used in quantization aware training. |
| [`ConvBnReLU1d`](generated/torch.ao.nn.intrinsic.qat.ConvBnReLU1d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU1d "torch.ao.nn.intrinsic.qat.ConvBnReLU1d") | A ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training. |
| [`ConvBn2d`](generated/torch.ao.nn.intrinsic.qat.ConvBn2d.html#torch.ao.nn.intrinsic.qat.ConvBn2d "torch.ao.nn.intrinsic.qat.ConvBn2d") | A ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training. |
| [`ConvBnReLU2d`](generated/torch.ao.nn.intrinsic.qat.ConvBnReLU2d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU2d "torch.ao.nn.intrinsic.qat.ConvBnReLU2d") | A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training. |
| [`ConvReLU2d`](generated/torch.ao.nn.intrinsic.qat.ConvReLU2d.html#torch.ao.nn.intrinsic.qat.ConvReLU2d "torch.ao.nn.intrinsic.qat.ConvReLU2d") | A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training. |
| [`ConvBn3d`](generated/torch.ao.nn.intrinsic.qat.ConvBn3d.html#torch.ao.nn.intrinsic.qat.ConvBn3d "torch.ao.nn.intrinsic.qat.ConvBn3d") | A ConvBn3d module is a module fused from Conv3d and BatchNorm3d, attached with FakeQuantize modules for weight, used in quantization aware training. |
| [`ConvBnReLU3d`](generated/torch.ao.nn.intrinsic.qat.ConvBnReLU3d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU3d "torch.ao.nn.intrinsic.qat.ConvBnReLU3d") | A ConvBnReLU3d module is a module fused from Conv3d, BatchNorm3d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training. |
| [`ConvReLU3d`](generated/torch.ao.nn.intrinsic.qat.ConvReLU3d.html#torch.ao.nn.intrinsic.qat.ConvReLU3d "torch.ao.nn.intrinsic.qat.ConvReLU3d") | A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with FakeQuantize modules for weight for quantization aware training. |
| [`update_bn_stats`](generated/torch.ao.nn.intrinsic.qat.update_bn_stats.html#torch.ao.nn.intrinsic.qat.update_bn_stats "torch.ao.nn.intrinsic.qat.update_bn_stats") |  |
| [`freeze_bn_stats`](generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats.html#torch.ao.nn.intrinsic.qat.freeze_bn_stats "torch.ao.nn.intrinsic.qat.freeze_bn_stats") |  |

torch.ao.nn.intrinsic.quantized 
---------------------------------------------------------------------------------------------------------

This module implements the quantized implementations of fused operations
like conv + relu. No BatchNorm variants as it’s usually folded into convolution
for inference. 

| [`BNReLU2d`](generated/torch.ao.nn.intrinsic.quantized.BNReLU2d.html#torch.ao.nn.intrinsic.quantized.BNReLU2d "torch.ao.nn.intrinsic.quantized.BNReLU2d") | A BNReLU2d module is a fused module of BatchNorm2d and ReLU |
| --- | --- |
| [`BNReLU3d`](generated/torch.ao.nn.intrinsic.quantized.BNReLU3d.html#torch.ao.nn.intrinsic.quantized.BNReLU3d "torch.ao.nn.intrinsic.quantized.BNReLU3d") | A BNReLU3d module is a fused module of BatchNorm3d and ReLU |
| [`ConvReLU1d`](generated/torch.ao.nn.intrinsic.quantized.ConvReLU1d.html#torch.ao.nn.intrinsic.quantized.ConvReLU1d "torch.ao.nn.intrinsic.quantized.ConvReLU1d") | A ConvReLU1d module is a fused module of Conv1d and ReLU |
| [`ConvReLU2d`](generated/torch.ao.nn.intrinsic.quantized.ConvReLU2d.html#torch.ao.nn.intrinsic.quantized.ConvReLU2d "torch.ao.nn.intrinsic.quantized.ConvReLU2d") | A ConvReLU2d module is a fused module of Conv2d and ReLU |
| [`ConvReLU3d`](generated/torch.ao.nn.intrinsic.quantized.ConvReLU3d.html#torch.ao.nn.intrinsic.quantized.ConvReLU3d "torch.ao.nn.intrinsic.quantized.ConvReLU3d") | A ConvReLU3d module is a fused module of Conv3d and ReLU |
| [`LinearReLU`](generated/torch.ao.nn.intrinsic.quantized.LinearReLU.html#torch.ao.nn.intrinsic.quantized.LinearReLU "torch.ao.nn.intrinsic.quantized.LinearReLU") | A LinearReLU module fused from Linear and ReLU modules |

torch.ao.nn.intrinsic.quantized.dynamic 
-------------------------------------------------------------------------------------------------------------------------

This module implements the quantized dynamic implementations of fused operations
like linear + relu. 

| [`LinearReLU`](generated/torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU.html#torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU "torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU") | A LinearReLU module fused from Linear and ReLU modules that can be used for dynamic quantization. |
| --- | --- |

torch.ao.nn.qat 
-------------------------------------------------------------------------

This module implements versions of the key nn modules **Conv2d()** and **Linear()** which run in FP32 but with rounding applied to simulate the
effect of INT8 quantization. 

| [`Conv2d`](generated/torch.ao.nn.qat.Conv2d.html#torch.ao.nn.qat.Conv2d "torch.ao.nn.qat.Conv2d") | A Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training. |
| --- | --- |
| [`Conv3d`](generated/torch.ao.nn.qat.Conv3d.html#torch.ao.nn.qat.Conv3d "torch.ao.nn.qat.Conv3d") | A Conv3d module attached with FakeQuantize modules for weight, used for quantization aware training. |
| [`Linear`](generated/torch.ao.nn.qat.Linear.html#torch.ao.nn.qat.Linear "torch.ao.nn.qat.Linear") | A linear module attached with FakeQuantize modules for weight, used for quantization aware training. |

torch.ao.nn.qat.dynamic 
-----------------------------------------------------------------------------------------

This module implements versions of the key nn modules such as **Linear()** which run in FP32 but with rounding applied to simulate the effect of INT8
quantization and will be dynamically quantized during inference. 

| [`Linear`](generated/torch.ao.nn.qat.dynamic.Linear.html#torch.ao.nn.qat.dynamic.Linear "torch.ao.nn.qat.dynamic.Linear") | A linear module attached with FakeQuantize modules for weight, used for dynamic quantization aware training. |
| --- | --- |

torch.ao.nn.quantized 
---------------------------------------------------------------------------------------------

This module implements the quantized versions of the nn layers such as `~torch.nn.Conv2d`  and `torch.nn.ReLU`  . 

| [`ReLU6`](generated/torch.ao.nn.quantized.ReLU6.html#torch.ao.nn.quantized.ReLU6 "torch.ao.nn.quantized.ReLU6") | Applies the element-wise function: |
| --- | --- |
| [`Hardswish`](generated/torch.ao.nn.quantized.Hardswish.html#torch.ao.nn.quantized.Hardswish "torch.ao.nn.quantized.Hardswish") | This is the quantized version of [`Hardswish`](generated/torch.nn.Hardswish.html#torch.nn.Hardswish "torch.nn.Hardswish")  . |
| [`ELU`](generated/torch.ao.nn.quantized.ELU.html#torch.ao.nn.quantized.ELU "torch.ao.nn.quantized.ELU") | This is the quantized equivalent of [`ELU`](generated/torch.nn.ELU.html#torch.nn.ELU "torch.nn.ELU")  . |
| [`LeakyReLU`](generated/torch.ao.nn.quantized.LeakyReLU.html#torch.ao.nn.quantized.LeakyReLU "torch.ao.nn.quantized.LeakyReLU") | This is the quantized equivalent of [`LeakyReLU`](generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU "torch.nn.LeakyReLU")  . |
| [`Sigmoid`](generated/torch.ao.nn.quantized.Sigmoid.html#torch.ao.nn.quantized.Sigmoid "torch.ao.nn.quantized.Sigmoid") | This is the quantized equivalent of [`Sigmoid`](generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid "torch.nn.Sigmoid")  . |
| [`BatchNorm2d`](generated/torch.ao.nn.quantized.BatchNorm2d.html#torch.ao.nn.quantized.BatchNorm2d "torch.ao.nn.quantized.BatchNorm2d") | This is the quantized version of [`BatchNorm2d`](generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d "torch.nn.BatchNorm2d")  . |
| [`BatchNorm3d`](generated/torch.ao.nn.quantized.BatchNorm3d.html#torch.ao.nn.quantized.BatchNorm3d "torch.ao.nn.quantized.BatchNorm3d") | This is the quantized version of [`BatchNorm3d`](generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d "torch.nn.BatchNorm3d")  . |
| [`Conv1d`](generated/torch.ao.nn.quantized.Conv1d.html#torch.ao.nn.quantized.Conv1d "torch.ao.nn.quantized.Conv1d") | Applies a 1D convolution over a quantized input signal composed of several quantized input planes. |
| [`Conv2d`](generated/torch.ao.nn.quantized.Conv2d.html#torch.ao.nn.quantized.Conv2d "torch.ao.nn.quantized.Conv2d") | Applies a 2D convolution over a quantized input signal composed of several quantized input planes. |
| [`Conv3d`](generated/torch.ao.nn.quantized.Conv3d.html#torch.ao.nn.quantized.Conv3d "torch.ao.nn.quantized.Conv3d") | Applies a 3D convolution over a quantized input signal composed of several quantized input planes. |
| [`ConvTranspose1d`](generated/torch.ao.nn.quantized.ConvTranspose1d.html#torch.ao.nn.quantized.ConvTranspose1d "torch.ao.nn.quantized.ConvTranspose1d") | Applies a 1D transposed convolution operator over an input image composed of several input planes. |
| [`ConvTranspose2d`](generated/torch.ao.nn.quantized.ConvTranspose2d.html#torch.ao.nn.quantized.ConvTranspose2d "torch.ao.nn.quantized.ConvTranspose2d") | Applies a 2D transposed convolution operator over an input image composed of several input planes. |
| [`ConvTranspose3d`](generated/torch.ao.nn.quantized.ConvTranspose3d.html#torch.ao.nn.quantized.ConvTranspose3d "torch.ao.nn.quantized.ConvTranspose3d") | Applies a 3D transposed convolution operator over an input image composed of several input planes. |
| [`Embedding`](generated/torch.ao.nn.quantized.Embedding.html#torch.ao.nn.quantized.Embedding "torch.ao.nn.quantized.Embedding") | A quantized Embedding module with quantized packed weights as inputs. |
| [`EmbeddingBag`](generated/torch.ao.nn.quantized.EmbeddingBag.html#torch.ao.nn.quantized.EmbeddingBag "torch.ao.nn.quantized.EmbeddingBag") | A quantized EmbeddingBag module with quantized packed weights as inputs. |
| [`FloatFunctional`](generated/torch.ao.nn.quantized.FloatFunctional.html#torch.ao.nn.quantized.FloatFunctional "torch.ao.nn.quantized.FloatFunctional") | State collector class for float operations. |
| [`FXFloatFunctional`](generated/torch.ao.nn.quantized.FXFloatFunctional.html#torch.ao.nn.quantized.FXFloatFunctional "torch.ao.nn.quantized.FXFloatFunctional") | module to replace FloatFunctional module before FX graph mode quantization, since activation_post_process will be inserted in top level module directly |
| [`QFunctional`](generated/torch.ao.nn.quantized.QFunctional.html#torch.ao.nn.quantized.QFunctional "torch.ao.nn.quantized.QFunctional") | Wrapper class for quantized operations. |
| [`Linear`](generated/torch.ao.nn.quantized.Linear.html#torch.ao.nn.quantized.Linear "torch.ao.nn.quantized.Linear") | A quantized linear module with quantized tensor as inputs and outputs. |
| [`LayerNorm`](generated/torch.ao.nn.quantized.LayerNorm.html#torch.ao.nn.quantized.LayerNorm "torch.ao.nn.quantized.LayerNorm") | This is the quantized version of [`LayerNorm`](generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm "torch.nn.LayerNorm")  . |
| [`GroupNorm`](generated/torch.ao.nn.quantized.GroupNorm.html#torch.ao.nn.quantized.GroupNorm "torch.ao.nn.quantized.GroupNorm") | This is the quantized version of [`GroupNorm`](generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm "torch.nn.GroupNorm")  . |
| [`InstanceNorm1d`](generated/torch.ao.nn.quantized.InstanceNorm1d.html#torch.ao.nn.quantized.InstanceNorm1d "torch.ao.nn.quantized.InstanceNorm1d") | This is the quantized version of [`InstanceNorm1d`](generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d "torch.nn.InstanceNorm1d")  . |
| [`InstanceNorm2d`](generated/torch.ao.nn.quantized.InstanceNorm2d.html#torch.ao.nn.quantized.InstanceNorm2d "torch.ao.nn.quantized.InstanceNorm2d") | This is the quantized version of [`InstanceNorm2d`](generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d "torch.nn.InstanceNorm2d")  . |
| [`InstanceNorm3d`](generated/torch.ao.nn.quantized.InstanceNorm3d.html#torch.ao.nn.quantized.InstanceNorm3d "torch.ao.nn.quantized.InstanceNorm3d") | This is the quantized version of [`InstanceNorm3d`](generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d "torch.nn.InstanceNorm3d")  . |

torch.ao.nn.quantized.functional 
-----------------------------------------------------------------------------------------------------------

Functional interface (quantized). 

This module implements the quantized versions of the functional layers such as *~torch.nn.functional.conv2d* and *torch.nn.functional.relu* . Note: <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<mtext>
</mtext>
<mi>
           t
          </mi>
<mi>
           o
          </mi>
<mi>
           r
          </mi>
<mi>
           c
          </mi>
<mi>
           h
          </mi>
<mi mathvariant="normal">
           .
          </mi>
<mi>
           n
          </mi>
<mi>
           n
          </mi>
<mi mathvariant="normal">
           .
          </mi>
<mi>
           f
          </mi>
<mi>
           u
          </mi>
<mi>
           n
          </mi>
<mi>
           c
          </mi>
<mi>
           t
          </mi>
<mi>
           i
          </mi>
<mi>
           o
          </mi>
<mi>
           n
          </mi>
<mi>
           a
          </mi>
<mi>
           l
          </mi>
<mi mathvariant="normal">
           .
          </mi>
<mi>
           r
          </mi>
<mi>
           e
          </mi>
<mi>
           l
          </mi>
<mi>
           u
          </mi>
</mrow>
<annotation encoding="application/x-tex">
          ~torch.nn.functional.relu
         </annotation>
</semantics>
</math> -->t o r c h . n n . f u n c t i o n a l . r e l u ~torch.nn.functional.relut orc h . nn . f u n c t i o na l . re l u  supports quantized inputs. 

| [`avg_pool2d`](generated/torch.ao.nn.quantized.functional.avg_pool2d.html#torch.ao.nn.quantized.functional.avg_pool2d "torch.ao.nn.quantized.functional.avg_pool2d") | Applies 2D average-pooling operation in <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML"> <semantics> <mrow> <mi> k </mi> <mi> H </mi> <mo> × </mo> <mi> k </mi> <mi> W </mi> </mrow> <annotation encoding="application/x-tex"> kH times kW </annotation> </semantics> </math> -->k H × k W kH times kWk H × kW  regions by step size <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML"> <semantics> <mrow> <mi> s </mi> <mi> H </mi> <mo> × </mo> <mi> s </mi> <mi> W </mi> </mrow> <annotation encoding="application/x-tex"> sH times sW </annotation> </semantics> </math> -->s H × s W sH times sWsH × s W  steps. |
| --- | --- |
| [`avg_pool3d`](generated/torch.ao.nn.quantized.functional.avg_pool3d.html#torch.ao.nn.quantized.functional.avg_pool3d "torch.ao.nn.quantized.functional.avg_pool3d") | Applies 3D average-pooling operation in <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML"> <semantics> <mrow> <mi> k </mi> <mi> D </mi> <mtext> </mtext> <mi> t </mi> <mi> i </mi> <mi> m </mi> <mi> e </mi> <mi> s </mi> <mi> k </mi> <mi> H </mi> <mo> × </mo> <mi> k </mi> <mi> W </mi> </mrow> <annotation encoding="application/x-tex"> kD  times kH times kW </annotation> </semantics> </math> -->k D t i m e s k H × k W kD  times kH times kWk D t im es k H × kW  regions by step size <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML"> <semantics> <mrow> <mi> s </mi> <mi> D </mi> <mo> × </mo> <mi> s </mi> <mi> H </mi> <mo> × </mo> <mi> s </mi> <mi> W </mi> </mrow> <annotation encoding="application/x-tex"> sD times sH times sW </annotation> </semantics> </math> -->s D × s H × s W sD times sH times sWsD × sH × s W  steps. |
| [`adaptive_avg_pool2d`](generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d.html#torch.ao.nn.quantized.functional.adaptive_avg_pool2d "torch.ao.nn.quantized.functional.adaptive_avg_pool2d") | Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes. |
| [`adaptive_avg_pool3d`](generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d.html#torch.ao.nn.quantized.functional.adaptive_avg_pool3d "torch.ao.nn.quantized.functional.adaptive_avg_pool3d") | Applies a 3D adaptive average pooling over a quantized input signal composed of several quantized input planes. |
| [`conv1d`](generated/torch.ao.nn.quantized.functional.conv1d.html#torch.ao.nn.quantized.functional.conv1d "torch.ao.nn.quantized.functional.conv1d") | Applies a 1D convolution over a quantized 1D input composed of several input planes. |
| [`conv2d`](generated/torch.ao.nn.quantized.functional.conv2d.html#torch.ao.nn.quantized.functional.conv2d "torch.ao.nn.quantized.functional.conv2d") | Applies a 2D convolution over a quantized 2D input composed of several input planes. |
| [`conv3d`](generated/torch.ao.nn.quantized.functional.conv3d.html#torch.ao.nn.quantized.functional.conv3d "torch.ao.nn.quantized.functional.conv3d") | Applies a 3D convolution over a quantized 3D input composed of several input planes. |
| [`interpolate`](generated/torch.ao.nn.quantized.functional.interpolate.html#torch.ao.nn.quantized.functional.interpolate "torch.ao.nn.quantized.functional.interpolate") | Down/up samples the input to either the given `size`  or the given `scale_factor` |
| [`linear`](generated/torch.ao.nn.quantized.functional.linear.html#torch.ao.nn.quantized.functional.linear "torch.ao.nn.quantized.functional.linear") | Applies a linear transformation to the incoming quantized data: <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML"> <semantics> <mrow> <mi> y </mi> <mo> = </mo> <mi> x </mi> <msup> <mi> A </mi> <mi> T </mi> </msup> <mo> + </mo> <mi> b </mi> </mrow> <annotation encoding="application/x-tex"> y = xA^T + b </annotation> </semantics> </math> -->y = x A T + b y = xA^T + by = x A T + b  . |
| [`max_pool1d`](generated/torch.ao.nn.quantized.functional.max_pool1d.html#torch.ao.nn.quantized.functional.max_pool1d "torch.ao.nn.quantized.functional.max_pool1d") | Applies a 1D max pooling over a quantized input signal composed of several quantized input planes. |
| [`max_pool2d`](generated/torch.ao.nn.quantized.functional.max_pool2d.html#torch.ao.nn.quantized.functional.max_pool2d "torch.ao.nn.quantized.functional.max_pool2d") | Applies a 2D max pooling over a quantized input signal composed of several quantized input planes. |
| [`celu`](generated/torch.ao.nn.quantized.functional.celu.html#torch.ao.nn.quantized.functional.celu "torch.ao.nn.quantized.functional.celu") | Applies the quantized CELU function element-wise. |
| [`leaky_relu`](generated/torch.ao.nn.quantized.functional.leaky_relu.html#torch.ao.nn.quantized.functional.leaky_relu "torch.ao.nn.quantized.functional.leaky_relu") | Quantized version of the. |
| [`hardtanh`](generated/torch.ao.nn.quantized.functional.hardtanh.html#torch.ao.nn.quantized.functional.hardtanh "torch.ao.nn.quantized.functional.hardtanh") | This is the quantized version of [`hardtanh()`](generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh "torch.nn.functional.hardtanh")  . |
| [`hardswish`](generated/torch.ao.nn.quantized.functional.hardswish.html#torch.ao.nn.quantized.functional.hardswish "torch.ao.nn.quantized.functional.hardswish") | This is the quantized version of [`hardswish()`](generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish "torch.nn.functional.hardswish")  . |
| [`threshold`](generated/torch.ao.nn.quantized.functional.threshold.html#torch.ao.nn.quantized.functional.threshold "torch.ao.nn.quantized.functional.threshold") | Applies the quantized version of the threshold function element-wise: |
| [`elu`](generated/torch.ao.nn.quantized.functional.elu.html#torch.ao.nn.quantized.functional.elu "torch.ao.nn.quantized.functional.elu") | This is the quantized version of [`elu()`](generated/torch.nn.functional.elu.html#torch.nn.functional.elu "torch.nn.functional.elu")  . |
| [`hardsigmoid`](generated/torch.ao.nn.quantized.functional.hardsigmoid.html#torch.ao.nn.quantized.functional.hardsigmoid "torch.ao.nn.quantized.functional.hardsigmoid") | This is the quantized version of [`hardsigmoid()`](generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid "torch.nn.functional.hardsigmoid")  . |
| [`clamp`](generated/torch.ao.nn.quantized.functional.clamp.html#torch.ao.nn.quantized.functional.clamp "torch.ao.nn.quantized.functional.clamp") | float(input, min_, max_) -> Tensor |
| [`upsample`](generated/torch.ao.nn.quantized.functional.upsample.html#torch.ao.nn.quantized.functional.upsample "torch.ao.nn.quantized.functional.upsample") | Upsamples the input to either the given `size`  or the given `scale_factor` |
| [`upsample_bilinear`](generated/torch.ao.nn.quantized.functional.upsample_bilinear.html#torch.ao.nn.quantized.functional.upsample_bilinear "torch.ao.nn.quantized.functional.upsample_bilinear") | Upsamples the input, using bilinear upsampling. |
| [`upsample_nearest`](generated/torch.ao.nn.quantized.functional.upsample_nearest.html#torch.ao.nn.quantized.functional.upsample_nearest "torch.ao.nn.quantized.functional.upsample_nearest") | Upsamples the input, using nearest neighbours' pixel values. |

torch.ao.nn.quantizable 
----------------------------------------------------------------------------------

This module implements the quantizable versions of some of the nn layers.
These modules can be used in conjunction with the custom module mechanism,
by providing the `custom_module_config`  argument to both prepare and convert. 

| [`LSTM`](generated/torch.ao.nn.quantizable.LSTM.html#torch.ao.nn.quantizable.LSTM "torch.ao.nn.quantizable.LSTM") | A quantizable long short-term memory (LSTM). |
| --- | --- |
| [`MultiheadAttention`](generated/torch.ao.nn.quantizable.MultiheadAttention.html#torch.ao.nn.quantizable.MultiheadAttention "torch.ao.nn.quantizable.MultiheadAttention") |  |

torch.ao.nn.quantized.dynamic 
-----------------------------------------------------------------------------------------------------

Dynamically quantized [`Linear`](generated/torch.nn.Linear.html#torch.nn.Linear "torch.nn.Linear")  , [`LSTM`](generated/torch.nn.LSTM.html#torch.nn.LSTM "torch.nn.LSTM")  , [`LSTMCell`](generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell "torch.nn.LSTMCell")  , [`GRUCell`](generated/torch.nn.GRUCell.html#torch.nn.GRUCell "torch.nn.GRUCell")  , and [`RNNCell`](generated/torch.nn.RNNCell.html#torch.nn.RNNCell "torch.nn.RNNCell")  . 

| [`Linear`](generated/torch.ao.nn.quantized.dynamic.Linear.html#torch.ao.nn.quantized.dynamic.Linear "torch.ao.nn.quantized.dynamic.Linear") | A dynamic quantized linear module with floating point tensor as inputs and outputs. |
| --- | --- |
| [`LSTM`](generated/torch.ao.nn.quantized.dynamic.LSTM.html#torch.ao.nn.quantized.dynamic.LSTM "torch.ao.nn.quantized.dynamic.LSTM") | A dynamic quantized LSTM module with floating point tensor as inputs and outputs. |
| [`GRU`](generated/torch.ao.nn.quantized.dynamic.GRU.html#torch.ao.nn.quantized.dynamic.GRU "torch.ao.nn.quantized.dynamic.GRU") | Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. |
| [`RNNCell`](generated/torch.ao.nn.quantized.dynamic.RNNCell.html#torch.ao.nn.quantized.dynamic.RNNCell "torch.ao.nn.quantized.dynamic.RNNCell") | An Elman RNN cell with tanh or ReLU non-linearity. |
| [`LSTMCell`](generated/torch.ao.nn.quantized.dynamic.LSTMCell.html#torch.ao.nn.quantized.dynamic.LSTMCell "torch.ao.nn.quantized.dynamic.LSTMCell") | A long short-term memory (LSTM) cell. |
| [`GRUCell`](generated/torch.ao.nn.quantized.dynamic.GRUCell.html#torch.ao.nn.quantized.dynamic.GRUCell "torch.ao.nn.quantized.dynamic.GRUCell") | A gated recurrent unit (GRU) cell |

Quantized dtypes and quantization schemes 
----------------------------------------------------------------------------------------------------------------------

Note that operator implementations currently only
support per channel quantization for weights of the **conv** and **linear** operators. Furthermore, the input data is
mapped linearly to the quantized data and vice versa
as follows: 

> <!-- MathML: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
> <semantics>
> <mtable columnalign="right left" columnspacing="0em" rowspacing="0.25em">
> <mtr>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mtext>
> Quantization:
> </mtext>
> </mstyle>
> </mtd>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> </mrow>
> </mstyle>
> </mtd>
> </mtr>
> <mtr>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> </mrow>
> </mstyle>
> </mtd>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> <mrow>
> </mrow>
> <msub>
> <mi>
> Q
> </mi>
> <mtext>
> out
> </mtext>
> </msub>
> <mo>
> =
> </mo>
> <mtext>
> clamp
> </mtext>
> <mo stretchy="false">
> (
> </mo>
> <msub>
> <mi>
> x
> </mi>
> <mtext>
> input
> </mtext>
> </msub>
> <mi mathvariant="normal">
> /
> </mi>
> <mi>
> s
> </mi>
> <mo>
> +
> </mo>
> <mi>
> z
> </mi>
> <mo separator="true">
> ,
> </mo>
> <msub>
> <mi>
> Q
> </mi>
> <mtext>
> min
> </mtext>
> </msub>
> <mo separator="true">
> ,
> </mo>
> <msub>
> <mi>
> Q
> </mi>
> <mtext>
> max
> </mtext>
> </msub>
> <mo stretchy="false">
> )
> </mo>
> </mrow>
> </mstyle>
> </mtd>
> </mtr>
> <mtr>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mtext>
> Dequantization:
> </mtext>
> </mstyle>
> </mtd>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> </mrow>
> </mstyle>
> </mtd>
> </mtr>
> <mtr>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> </mrow>
> </mstyle>
> </mtd>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> <mrow>
> </mrow>
> <msub>
> <mi>
> x
> </mi>
> <mtext>
> out
> </mtext>
> </msub>
> <mo>
> =
> </mo>
> <mo stretchy="false">
> (
> </mo>
> <msub>
> <mi>
> Q
> </mi>
> <mtext>
> input
> </mtext>
> </msub>
> <mo>
> −
> </mo>
> <mi>
> z
> </mi>
> <mo stretchy="false">
> )
> </mo>
> <mo>
> ∗
> </mo>
> <mi>
> s
> </mi>
> </mrow>
> </mstyle>
> </mtd>
> </mtr>
> </mtable>
> <annotation encoding="application/x-tex">
> begin{aligned}
> text{Quantization:}&amp;
> &amp;Q_text{out} = text{clamp}(x_text{input}/s+z, Q_text{min}, Q_text{max})
> text{Dequantization:}&amp;
> &amp;x_text{out} = (Q_text{input}-z)*s
> end{aligned}
> </annotation>
> </semantics>
> </math> -->
> Quantization: Q out = clamp ( x input / s + z , Q min , Q max ) Dequantization: x out = ( Q input − z ) ∗ s begin{aligned}
>  text{Quantization:}&
>  &Q_text{out} = text{clamp}(x_text{input}/s+z, Q_text{min}, Q_text{max})
>  text{Dequantization:}&
>  &x_text{out} = (Q_text{input}-z)*s
> end{aligned}
> 
> Quantization: Dequantization: ​ Q out ​ = clamp ( x input ​ / s + z , Q min ​ , Q max ​ ) x out ​ = ( Q input ​ − z ) ∗ s ​

where <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<mtext>
           clamp
          </mtext>
<mo stretchy="false">
           (
          </mo>
<mi mathvariant="normal">
           .
          </mi>
<mo stretchy="false">
           )
          </mo>
</mrow>
<annotation encoding="application/x-tex">
          text{clamp}(.)
         </annotation>
</semantics>
</math> -->clamp ( . ) text{clamp}(.)clamp ( . )  is the same as [`clamp()`](generated/torch.clamp.html#torch.clamp "torch.clamp")  while the
scale <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<mi>
           s
          </mi>
</mrow>
<annotation encoding="application/x-tex">
          s
         </annotation>
</semantics>
</math> -->s ss  and zero point <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<mi>
           z
          </mi>
</mrow>
<annotation encoding="application/x-tex">
          z
         </annotation>
</semantics>
</math> -->z zz  are then computed
as described in [`MinMaxObserver`](generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver "torch.ao.quantization.observer.MinMaxObserver")  , specifically: 

> <!-- MathML: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
> <semantics>
> <mtable columnalign="right left" columnspacing="0em" rowspacing="0.25em">
> <mtr>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mtext>
> if Symmetric:
> </mtext>
> </mstyle>
> </mtd>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> </mrow>
> </mstyle>
> </mtd>
> </mtr>
> <mtr>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> </mrow>
> </mstyle>
> </mtd>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> <mrow>
> </mrow>
> <mi>
> s
> </mi>
> <mo>
> =
> </mo>
> <mn>
> 2
> </mn>
> <mi>
> max
> </mi>
> <mo>
> ⁡
> </mo>
> <mo stretchy="false">
> (
> </mo>
> <mi mathvariant="normal">
> ∣
> </mi>
> <msub>
> <mi>
> x
> </mi>
> <mtext>
> min
> </mtext>
> </msub>
> <mi mathvariant="normal">
> ∣
> </mi>
> <mo separator="true">
> ,
> </mo>
> <msub>
> <mi>
> x
> </mi>
> <mtext>
> max
> </mtext>
> </msub>
> <mo stretchy="false">
> )
> </mo>
> <mi mathvariant="normal">
> /
> </mi>
> <mrow>
> <mo fence="true">
> (
> </mo>
> <msub>
> <mi>
> Q
> </mi>
> <mtext>
> max
> </mtext>
> </msub>
> <mo>
> −
> </mo>
> <msub>
> <mi>
> Q
> </mi>
> <mtext>
> min
> </mtext>
> </msub>
> <mo fence="true">
> )
> </mo>
> </mrow>
> </mrow>
> </mstyle>
> </mtd>
> </mtr>
> <mtr>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> </mrow>
> </mstyle>
> </mtd>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> <mrow>
> </mrow>
> <mi>
> z
> </mi>
> <mo>
> =
> </mo>
> <mrow>
> <mo fence="true">
> {
> </mo>
> <mtable columnalign="left left" columnspacing="1em" rowspacing="0.36em">
> <mtr>
> <mtd>
> <mstyle displaystyle="false" scriptlevel="0">
> <mn>
> 0
> </mn>
> </mstyle>
> </mtd>
> <mtd>
> <mstyle displaystyle="false" scriptlevel="0">
> <mtext>
> if dtype is qint8
> </mtext>
> </mstyle>
> </mtd>
> </mtr>
> <mtr>
> <mtd>
> <mstyle displaystyle="false" scriptlevel="0">
> <mn>
> 128
> </mn>
> </mstyle>
> </mtd>
> <mtd>
> <mstyle displaystyle="false" scriptlevel="0">
> <mtext>
> otherwise
> </mtext>
> </mstyle>
> </mtd>
> </mtr>
> </mtable>
> </mrow>
> </mrow>
> </mstyle>
> </mtd>
> </mtr>
> <mtr>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mtext>
> Otherwise:
> </mtext>
> </mstyle>
> </mtd>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> </mrow>
> </mstyle>
> </mtd>
> </mtr>
> <mtr>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> </mrow>
> </mstyle>
> </mtd>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> <mrow>
> </mrow>
> <mi>
> s
> </mi>
> <mo>
> =
> </mo>
> <mrow>
> <mo fence="true">
> (
> </mo>
> <msub>
> <mi>
> x
> </mi>
> <mtext>
> max
> </mtext>
> </msub>
> <mo>
> −
> </mo>
> <msub>
> <mi>
> x
> </mi>
> <mtext>
> min
> </mtext>
> </msub>
> <mo fence="true">
> )
> </mo>
> </mrow>
> <mi mathvariant="normal">
> /
> </mi>
> <mrow>
> <mo fence="true">
> (
> </mo>
> <msub>
> <mi>
> Q
> </mi>
> <mtext>
> max
> </mtext>
> </msub>
> <mo>
> −
> </mo>
> <msub>
> <mi>
> Q
> </mi>
> <mtext>
> min
> </mtext>
> </msub>
> <mo fence="true">
> )
> </mo>
> </mrow>
> </mrow>
> </mstyle>
> </mtd>
> </mtr>
> <mtr>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> </mrow>
> </mstyle>
> </mtd>
> <mtd>
> <mstyle displaystyle="true" scriptlevel="0">
> <mrow>
> <mrow>
> </mrow>
> <mi>
> z
> </mi>
> <mo>
> =
> </mo>
> <msub>
> <mi>
> Q
> </mi>
> <mtext>
> min
> </mtext>
> </msub>
> <mo>
> −
> </mo>
> <mtext>
> round
> </mtext>
> <mo stretchy="false">
> (
> </mo>
> <msub>
> <mi>
> x
> </mi>
> <mtext>
> min
> </mtext>
> </msub>
> <mi mathvariant="normal">
> /
> </mi>
> <mi>
> s
> </mi>
> <mo stretchy="false">
> )
> </mo>
> </mrow>
> </mstyle>
> </mtd>
> </mtr>
> </mtable>
> <annotation encoding="application/x-tex">
> begin{aligned}
> text{if Symmetric:}&amp;
> &amp;s = 2 max(|x_text{min}|, x_text{max}) /
> left( Q_text{max} - Q_text{min} right) 
> &amp;z = begin{cases}
> 0 &amp; text{if dtype is qint8} 
> 128 &amp; text{otherwise}
> end{cases}
> text{Otherwise:}&amp;
> &amp;s = left( x_text{max} - x_text{min}  right ) /
> left( Q_text{max} - Q_text{min} right ) 
> &amp;z = Q_text{min} - text{round}(x_text{min} / s)
> end{aligned}
> </annotation>
> </semantics>
> </math> -->
> if Symmetric: s = 2 max ⁡ ( ∣ x min ∣ , x max ) / ( Q max − Q min ) z = { 0 if dtype is qint8 128 otherwise Otherwise: s = ( x max − x min ) / ( Q max − Q min ) z = Q min − round ( x min / s ) begin{aligned}
>  text{if Symmetric:}&
>  &s = 2 max(|x_text{min}|, x_text{max}) /
>  left( Q_text{max} - Q_text{min} right) 
>  &z = begin{cases}
>  0 & text{if dtype is qint8} 
>  128 & text{otherwise}
>  end{cases}
>  text{Otherwise:}&
>  &s = left( x_text{max} - x_text{min} right ) /
>  left( Q_text{max} - Q_text{min} right ) 
>  &z = Q_text{min} - text{round}(x_text{min} / s)
> end{aligned}
> 
> if Symmetric: Otherwise: ​ s = 2 max ( ∣ x min ​ ∣ , x max ​ ) / ( Q max ​ − Q min ​ ) z = { 0 128 ​ if dtype is qint8 otherwise ​ s = ( x max ​ − x min ​ ) / ( Q max ​ − Q min ​ ) z = Q min ​ − round ( x min ​ / s ) ​

where :math: `[x_text{min}, x_text{max}]`  denotes the range of the input data while
:math: `Q_text{min}`  and :math: `Q_text{max}`  are respectively the minimum and maximum values of the quantized dtype. 

Note that the choice of :math: `s`  and :math: `z`  implies that zero is represented with no quantization error whenever zero is within
the range of the input data or symmetric quantization is being used. 

Additional data types and quantization schemes can be implemented through
the `custom operator mechanism <https://localhost:8000/tutorials/advanced/torch_script_custom_ops.html>`  _. 

* `torch.qscheme`  — Type to describe the quantization scheme of a tensor.
Supported types:

    + `torch.per_tensor_affine`  — per tensor, asymmetric
        + `torch.per_channel_affine`  — per channel, asymmetric
        + `torch.per_tensor_symmetric`  — per tensor, symmetric
        + `torch.per_channel_symmetric`  — per channel, symmetric
* `torch.dtype`  — Type to describe the data. Supported types:

    + `torch.quint8`  — 8-bit unsigned integer
        + `torch.qint8`  — 8-bit signed integer
        + `torch.qint32`  — 32-bit signed integer

QAT Modules. 

This package is in the process of being deprecated.
Please, use *torch.ao.nn.qat.modules* instead. 

QAT Dynamic Modules. 

This package is in the process of being deprecated.
Please, use *torch.ao.nn.qat.dynamic* instead. 

This file is in the process of migration to *torch/ao/quantization* , and
is kept here for compatibility while the migration process is ongoing.
If you are adding a new entry/functionality, please, add it to the
appropriate files under *torch/ao/quantization/fx/* , while adding an import statement
here. 

QAT Dynamic Modules. 

This package is in the process of being deprecated.
Please, use *torch.ao.nn.qat.dynamic* instead. 

Quantized Modules. 

Note::
:   The *torch.nn.quantized* namespace is in the process of being deprecated.
Please, use *torch.ao.nn.quantized* instead.

Quantized Dynamic Modules. 

This file is in the process of migration to *torch/ao/nn/quantized/dynamic* ,
and is kept here for compatibility while the migration process is ongoing.
If you are adding a new entry/functionality, please, add it to the
appropriate file under the *torch/ao/nn/quantized/dynamic* ,
while adding an import statement here.

