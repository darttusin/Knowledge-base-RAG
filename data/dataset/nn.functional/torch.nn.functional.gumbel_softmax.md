torch.nn.functional.gumbel_softmax 
=========================================================================================================

torch.nn.functional. gumbel_softmax ( *logits*  , *tau = 1*  , *hard = False*  , *eps = 1e-10*  , *dim = -1* ) [source](https://github.com/pytorch/pytorch/blob/v2.8.0/torch/nn/functional.py#L2143) 
:   Sample from the Gumbel-Softmax distribution ( [Link 1](https://arxiv.org/abs/1611.00712) [Link 2](https://arxiv.org/abs/1611.01144)  ) and optionally discretize. 

Parameters
:   * **logits** ( [*Tensor*](../tensors.html#torch.Tensor "torch.Tensor")  ) – *[…, num_features]* unnormalized log probabilities
* **tau** ( [*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")  ) – non-negative scalar temperature
* **hard** ( [*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")  ) – if `True`  , the returned samples will be discretized as one-hot vectors,
but will be differentiated as if it is the soft sample in autograd
* **dim** ( [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")  ) – A dimension along which softmax will be computed. Default: -1.

Returns
:   Sampled tensor of same shape as *logits* from the Gumbel-Softmax distribution.
If `hard=True`  , the returned samples will be one-hot, otherwise they will
be probability distributions that sum to 1 across *dim* .

Return type
:   [*Tensor*](../tensors.html#torch.Tensor "torch.Tensor")

Note 

This function is here for legacy reasons, may be removed from nn.Functional in the future.

Note 

The main trick for *hard* is to do *y_hard - y_soft.detach() + y_soft*

It achieves two things:
- makes the output value exactly one-hot
(since we add then subtract y_soft value)
- makes the gradient equal to y_soft gradient
(since we strip all other gradients)

Examples::
:   ```
>>> logits = torch.randn(20, 32)
>>> # Sample soft categorical using reparametrization trick:
>>> F.gumbel_softmax(logits, tau=1, hard=False)
>>> # Sample hard categorical using "Straight-through" trick:
>>> F.gumbel_softmax(logits, tau=1, hard=True)

```

