KLDivLoss 
======================================================

*class* torch.nn. KLDivLoss ( *size_average = None*  , *reduce = None*  , *reduction = 'mean'*  , *log_target = False* ) [source](https://github.com/pytorch/pytorch/blob/v2.8.0/torch/nn/modules/loss.py#L453) 
:   The Kullback-Leibler divergence loss. 

For tensors of the same shape <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<msub>
<mi>
             y
            </mi>
<mtext>
             pred
            </mtext>
</msub>
<mo separator="true">
            ,
           </mo>
<mtext>
</mtext>
<msub>
<mi>
             y
            </mi>
<mtext>
             true
            </mtext>
</msub>
</mrow>
<annotation encoding="application/x-tex">
           y_{text{pred}}, y_{text{true}}
          </annotation>
</semantics>
</math> -->y pred , y true y_{text{pred}}, y_{text{true}}y pred ​ , y true ​  ,
where <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<msub>
<mi>
             y
            </mi>
<mtext>
             pred
            </mtext>
</msub>
</mrow>
<annotation encoding="application/x-tex">
           y_{text{pred}}
          </annotation>
</semantics>
</math> -->y pred y_{text{pred}}y pred ​  is the `input`  and <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<msub>
<mi>
             y
            </mi>
<mtext>
             true
            </mtext>
</msub>
</mrow>
<annotation encoding="application/x-tex">
           y_{text{true}}
          </annotation>
</semantics>
</math> -->y true y_{text{true}}y true ​  is the `target`  , we define the **pointwise KL-divergence** as 

<!-- MathML: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<mi>
            L
           </mi>
<mo stretchy="false">
            (
           </mo>
<msub>
<mi>
             y
            </mi>
<mtext>
             pred
            </mtext>
</msub>
<mo separator="true">
            ,
           </mo>
<mtext>
</mtext>
<msub>
<mi>
             y
            </mi>
<mtext>
             true
            </mtext>
</msub>
<mo stretchy="false">
            )
           </mo>
<mo>
            =
           </mo>
<msub>
<mi>
             y
            </mi>
<mtext>
             true
            </mtext>
</msub>
<mo>
            ⋅
           </mo>
<mi>
            log
           </mi>
<mo>
            ⁡
           </mo>
<mfrac>
<msub>
<mi>
              y
             </mi>
<mtext>
              true
             </mtext>
</msub>
<msub>
<mi>
              y
             </mi>
<mtext>
              pred
             </mtext>
</msub>
</mfrac>
<mo>
            =
           </mo>
<msub>
<mi>
             y
            </mi>
<mtext>
             true
            </mtext>
</msub>
<mo>
            ⋅
           </mo>
<mo stretchy="false">
            (
           </mo>
<mi>
            log
           </mi>
<mo>
            ⁡
           </mo>
<msub>
<mi>
             y
            </mi>
<mtext>
             true
            </mtext>
</msub>
<mo>
            −
           </mo>
<mi>
            log
           </mi>
<mo>
            ⁡
           </mo>
<msub>
<mi>
             y
            </mi>
<mtext>
             pred
            </mtext>
</msub>
<mo stretchy="false">
            )
           </mo>
</mrow>
<annotation encoding="application/x-tex">
           L(y_{text{pred}}, y_{text{true}})
    = y_{text{true}} cdot log frac{y_{text{true}}}{y_{text{pred}}}
    = y_{text{true}} cdot (log y_{text{true}} - log y_{text{pred}})
          </annotation>
</semantics>
</math> -->
L ( y pred , y true ) = y true ⋅ log ⁡ y true y pred = y true ⋅ ( log ⁡ y true − log ⁡ y pred ) L(y_{text{pred}}, y_{text{true}})
 = y_{text{true}} cdot log frac{y_{text{true}}}{y_{text{pred}}}
 = y_{text{true}} cdot (log y_{text{true}} - log y_{text{pred}})

L ( y pred ​ , y true ​ ) = y true ​ ⋅ lo g y pred ​ y true ​ ​ = y true ​ ⋅ ( lo g y true ​ − lo g y pred ​ )

To avoid underflow issues when computing this quantity, this loss expects the argument `input`  in the log-space. The argument `target`  may also be provided in the
log-space if `log_target` *= True* . 

To summarise, this function is roughly equivalent to computing 

```
if not log_target:  # default
    loss_pointwise = target * (target.log() - input)
else:
    loss_pointwise = target.exp() * (target - input)

```

and then reducing this result depending on the argument `reduction`  as 

```
if reduction == "mean":  # default
    loss = loss_pointwise.mean()
elif reduction == "batchmean":  # mathematically correct
    loss = loss_pointwise.sum() / input.size(0)
elif reduction == "sum":
    loss = loss_pointwise.sum()
else:  # reduction == "none"
    loss = loss_pointwise

```

Note 

As all the other losses in PyTorch, this function expects the first argument, `input`  , to be the output of the model (e.g. the neural network)
and the second, `target`  , to be the observations in the dataset.
This differs from the standard mathematical notation <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<mi>
             K
            </mi>
<mi>
             L
            </mi>
<mo stretchy="false">
             (
            </mo>
<mi>
             P
            </mi>
<mtext>
</mtext>
<mi mathvariant="normal">
             ∣
            </mi>
<mi mathvariant="normal">
             ∣
            </mi>
<mtext>
</mtext>
<mi>
             Q
            </mi>
<mo stretchy="false">
             )
            </mo>
</mrow>
<annotation encoding="application/x-tex">
            KL(P || Q)
           </annotation>
</semantics>
</math> -->K L ( P ∣ ∣ Q ) KL(P || Q)K L ( P ∣∣ Q )  where <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<mi>
             P
            </mi>
</mrow>
<annotation encoding="application/x-tex">
            P
           </annotation>
</semantics>
</math> -->P PP  denotes the distribution of the observations and <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<mi>
             Q
            </mi>
</mrow>
<annotation encoding="application/x-tex">
            Q
           </annotation>
</semantics>
</math> -->Q QQ  denotes the model.

Warning 

`reduction` *= “mean”* doesn’t return the true KL divergence value, please use `reduction` *= “batchmean”* which aligns with the mathematical definition.

Parameters
:   * **size_average** ( [*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *,* *optional*  ) – Deprecated (see `reduction`  ). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field `size_average`  is set to *False* , the losses are instead summed for each minibatch. Ignored
when `reduce`  is *False* . Default: *True*
* **reduce** ( [*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *,* *optional*  ) – Deprecated (see `reduction`  ). By default, the
losses are averaged or summed over observations for each minibatch depending
on `size_average`  . When `reduce`  is *False* , returns a loss per
batch element instead and ignores `size_average`  . Default: *True*
* **reduction** ( [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") *,* *optional*  ) – Specifies the reduction to apply to the output. Default: *“mean”*
* **log_target** ( [*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") *,* *optional*  ) – Specifies whether *target* is the log space. Default: *False*

Shape:
:   * Input: <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<mo stretchy="false">
                (
               </mo>
<mo>
                ∗
               </mo>
<mo stretchy="false">
                )
               </mo>
</mrow>
<annotation encoding="application/x-tex">
               (*)
              </annotation>
</semantics>
</math> -->( ∗ ) (*)( ∗ )  , where <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<mo>
                ∗
               </mo>
</mrow>
<annotation encoding="application/x-tex">
               *
              </annotation>
</semantics>
</math> -->∗ *∗  means any number of dimensions.

* Target: <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<mo stretchy="false">
                (
               </mo>
<mo>
                ∗
               </mo>
<mo stretchy="false">
                )
               </mo>
</mrow>
<annotation encoding="application/x-tex">
               (*)
              </annotation>
</semantics>
</math> -->( ∗ ) (*)( ∗ )  , same shape as the input.

* Output: scalar by default. If `reduction`  is *‘none’* , then <!-- MathML: <math xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow>
<mo stretchy="false">
                (
               </mo>
<mo>
                ∗
               </mo>
<mo stretchy="false">
                )
               </mo>
</mrow>
<annotation encoding="application/x-tex">
               (*)
              </annotation>
</semantics>
</math> -->( ∗ ) (*)( ∗ )  ,
same shape as the input.

Examples 

```
>>> kl_loss = nn.KLDivLoss(reduction="batchmean")
>>> # input should be a distribution in the log space
>>> input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)
>>> # Sample a batch of distributions. Usually this would come from the dataset
>>> target = F.softmax(torch.rand(3, 5), dim=1)
>>> output = kl_loss(input, target)
>>>
>>> kl_loss = nn.KLDivLoss(reduction="batchmean", log_target=True)
>>> log_target = F.log_softmax(torch.rand(3, 5), dim=1)
>>> output = kl_loss(input, log_target)

```

