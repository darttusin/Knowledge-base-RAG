torch.cuda.memory.get_allocator_backend 
====================================================================================================================

torch.cuda.memory. get_allocator_backend ( ) [source](https://github.com/pytorch/pytorch/blob/v2.8.0/torch/cuda/memory.py#L1076) 
:   Return a string describing the active allocator backend as set by `PYTORCH_CUDA_ALLOC_CONF`  . Currently available backends are `native`  (PyTorch’s native caching allocator) and *cudaMallocAsync`* (CUDA’s built-in asynchronous allocator). 

Note 

See [Memory management](../notes/cuda.html#cuda-memory-management)  for details on choosing the allocator backend.

Return type
:   [str](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")

