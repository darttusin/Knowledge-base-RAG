torch.nn.utils.clip_grad_norm 
================================================================================================

torch.nn.utils. clip_grad_norm ( *parameters*  , *max_norm*  , *norm_type = 2.0*  , *error_if_nonfinite = False*  , *foreach = None* ) [source](https://github.com/pytorch/pytorch/blob/v2.8.0/torch/nn/utils/clip_grad.py#L226) 
:   Clip the gradient norm of an iterable of parameters. 

Warning 

This method is now deprecated in favor of [`torch.nn.utils.clip_grad_norm_()`](torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_ "torch.nn.utils.clip_grad_norm_")  .

Return type
:   [*Tensor*](../tensors.html#torch.Tensor "torch.Tensor")

